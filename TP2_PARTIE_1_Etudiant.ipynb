{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2. PARTIE 1. scikit-learn + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module de Machine Learning en Python : scitkit-learn (sklearn)\n",
    "http://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan :\n",
    "\n",
    "   [- Iris dataset](#1)\n",
    "   \n",
    "   [- Naive Bayes](#2)\n",
    "   \n",
    "   [- Mon Naive Bayes](#3)\n",
    "   \n",
    "   [- Tests](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher le dataset **iris** dans le module sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Analyser les résultats des commandes suivantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'],\n",
       "       dtype='<U10')}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.4,  3.7,  1.5,  0.2],\n",
       "       [ 4.8,  3.4,  1.6,  0.2],\n",
       "       [ 4.8,  3. ,  1.4,  0.1],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 5.4,  3.9,  1.3,  0.4],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 5.4,  3.4,  1.7,  0.2],\n",
       "       [ 5.1,  3.7,  1.5,  0.4],\n",
       "       [ 4.6,  3.6,  1. ,  0.2],\n",
       "       [ 5.1,  3.3,  1.7,  0.5],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.6,  0.2],\n",
       "       [ 4.8,  3.1,  1.6,  0.2],\n",
       "       [ 5.4,  3.4,  1.5,  0.4],\n",
       "       [ 5.2,  4.1,  1.5,  0.1],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5. ,  3.2,  1.2,  0.2],\n",
       "       [ 5.5,  3.5,  1.3,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 4.4,  3. ,  1.3,  0.2],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5. ,  3.5,  1.6,  0.6],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 5.1,  3.8,  1.6,  0.2],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5. ,  3.3,  1.4,  0.2],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 6.4,  3.2,  4.5,  1.5],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 5.5,  2.3,  4. ,  1.3],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 5.7,  2.8,  4.5,  1.3],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 6.6,  2.9,  4.6,  1.3],\n",
       "       [ 5.2,  2.7,  3.9,  1.4],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.9,  3. ,  4.2,  1.5],\n",
       "       [ 6. ,  2.2,  4. ,  1. ],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 6.7,  3.1,  4.4,  1.4],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 5.8,  2.7,  4.1,  1. ],\n",
       "       [ 6.2,  2.2,  4.5,  1.5],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 6.1,  2.8,  4. ,  1.3],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.4,  2.9,  4.3,  1.3],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6.8,  2.8,  4.8,  1.4],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.5,  2.4,  3.8,  1.1],\n",
       "       [ 5.5,  2.4,  3.7,  1. ],\n",
       "       [ 5.8,  2.7,  3.9,  1.2],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6. ,  3.4,  4.5,  1.6],\n",
       "       [ 6.7,  3.1,  4.7,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 5.6,  3. ,  4.1,  1.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 5.8,  2.6,  4. ,  1.2],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 5.6,  2.7,  4.2,  1.3],\n",
       "       [ 5.7,  3. ,  4.2,  1.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 5.7,  2.8,  4.1,  1.3],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 7.6,  3. ,  6.6,  2.1],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 6.7,  2.5,  5.8,  1.8],\n",
       "       [ 7.2,  3.6,  6.1,  2.5],\n",
       "       [ 6.5,  3.2,  5.1,  2. ],\n",
       "       [ 6.4,  2.7,  5.3,  1.9],\n",
       "       [ 6.8,  3. ,  5.5,  2.1],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8],\n",
       "       [ 6.7,  3.3,  5.7,  2.1],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 6.1,  3. ,  4.9,  1.8],\n",
       "       [ 6.4,  2.8,  5.6,  2.1],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 7.4,  2.8,  6.1,  1.9],\n",
       "       [ 7.9,  3.8,  6.4,  2. ],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 7.7,  3. ,  6.1,  2.3],\n",
       "       [ 6.3,  3.4,  5.6,  2.4],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 6.9,  3.1,  5.1,  2.3],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.8,  3.2,  5.9,  2.3],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 6.7,  3. ,  5.2,  2.3],\n",
       "       [ 6.3,  2.5,  5. ,  1.9],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 5.9,  3. ,  5.1,  1.8]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data\n",
    "#features : 150*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target\n",
    "#labels : vector 150*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GaussianNB in module sklearn.naive_bayes:\n",
      "\n",
      "class GaussianNB(BaseNB)\n",
      " |  Gaussian Naive Bayes (GaussianNB)\n",
      " |  \n",
      " |  Can perform online updates to model parameters via `partial_fit` method.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like, shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_prior_ : array, shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  class_count_ : array, shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  theta_ : array, shape (n_classes, n_features)\n",
      " |      mean of each feature per class\n",
      " |  \n",
      " |  sigma_ : array, shape (n_classes, n_features)\n",
      " |      variance of each feature per class\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB(priors=None)\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB(priors=None)\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      BaseNB\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, priors=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like, shape (n_classes,), optional (default=None)\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = [n_samples]\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module sklearn.naive_bayes:\n",
      "\n",
      "fit(X, y, sample_weight=None) method of sklearn.naive_bayes.GaussianNB instance\n",
      "    Fit Gaussian Naive Bayes according to X, y\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        Training vectors, where n_samples is the number of samples\n",
      "        and n_features is the number of features.\n",
      "    \n",
      "    y : array-like, shape (n_samples,)\n",
      "        Target values.\n",
      "    \n",
      "    sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      "        Weights applied to individual samples (1. for unweighted).\n",
      "    \n",
      "        .. versionadded:: 0.17\n",
      "           Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    self : object\n",
      "        Returns self.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "help(gnb.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On donne ici un exemple d'utilisation du Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (iris.data.shape[0],(iris.target != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.4,  3.7,  1.5,  0.2],\n",
       "       [ 4.8,  3.4,  1.6,  0.2],\n",
       "       [ 4.8,  3. ,  1.4,  0.1],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 5.4,  3.9,  1.3,  0.4],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 5.4,  3.4,  1.7,  0.2],\n",
       "       [ 5.1,  3.7,  1.5,  0.4],\n",
       "       [ 4.6,  3.6,  1. ,  0.2],\n",
       "       [ 5.1,  3.3,  1.7,  0.5],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.6,  0.2],\n",
       "       [ 4.8,  3.1,  1.6,  0.2],\n",
       "       [ 5.4,  3.4,  1.5,  0.4],\n",
       "       [ 5.2,  4.1,  1.5,  0.1],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5. ,  3.2,  1.2,  0.2],\n",
       "       [ 5.5,  3.5,  1.3,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 4.4,  3. ,  1.3,  0.2],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5. ,  3.5,  1.6,  0.6],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 5.1,  3.8,  1.6,  0.2],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5. ,  3.3,  1.4,  0.2],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 6.4,  3.2,  4.5,  1.5],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 5.5,  2.3,  4. ,  1.3],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 5.7,  2.8,  4.5,  1.3],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 6.6,  2.9,  4.6,  1.3],\n",
       "       [ 5.2,  2.7,  3.9,  1.4],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.9,  3. ,  4.2,  1.5],\n",
       "       [ 6. ,  2.2,  4. ,  1. ],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 6.7,  3.1,  4.4,  1.4],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 5.8,  2.7,  4.1,  1. ],\n",
       "       [ 6.2,  2.2,  4.5,  1.5],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 6.1,  2.8,  4. ,  1.3],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.4,  2.9,  4.3,  1.3],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6.8,  2.8,  4.8,  1.4],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.5,  2.4,  3.8,  1.1],\n",
       "       [ 5.5,  2.4,  3.7,  1. ],\n",
       "       [ 5.8,  2.7,  3.9,  1.2],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6. ,  3.4,  4.5,  1.6],\n",
       "       [ 6.7,  3.1,  4.7,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 5.6,  3. ,  4.1,  1.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 5.8,  2.6,  4. ,  1.2],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 5.6,  2.7,  4.2,  1.3],\n",
       "       [ 5.7,  3. ,  4.2,  1.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 5.7,  2.8,  4.1,  1.3],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 7.6,  3. ,  6.6,  2.1],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 6.7,  2.5,  5.8,  1.8],\n",
       "       [ 7.2,  3.6,  6.1,  2.5],\n",
       "       [ 6.5,  3.2,  5.1,  2. ],\n",
       "       [ 6.4,  2.7,  5.3,  1.9],\n",
       "       [ 6.8,  3. ,  5.5,  2.1],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8],\n",
       "       [ 6.7,  3.3,  5.7,  2.1],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 6.1,  3. ,  4.9,  1.8],\n",
       "       [ 6.4,  2.8,  5.6,  2.1],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 7.4,  2.8,  6.1,  1.9],\n",
       "       [ 7.9,  3.8,  6.4,  2. ],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 7.7,  3. ,  6.1,  2.3],\n",
       "       [ 6.3,  3.4,  5.6,  2.4],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 6.9,  3.1,  5.1,  2.3],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.8,  3.2,  5.9,  2.3],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 6.7,  3. ,  5.2,  2.3],\n",
       "       [ 6.3,  2.5,  5. ,  1.9],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 5.9,  3. ,  5.1,  1.8]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) On sépare en 2 parties égales le dataset avec tirage aléatoire sans remise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([122,  49,  44, 124, 119,  70,  98, 104,  64,  23,  11,  48, 128,\n",
       "       135,  82, 112, 111,  58, 129,  69,  50,  15,  24,  10,  36,  52,\n",
       "       116,  51, 145, 115,  57,  21, 146,   6,  32,  31, 108,  18,  45,\n",
       "        74, 120,  59,  68,  99,  39,  37,  87, 126,  34,  63, 132,  88,\n",
       "       105, 100,  71, 106, 147,  62,  80,  73, 101,  91,  65,  54,  79,\n",
       "       134,  60,  27,  28,   4,  41, 139, 113,  42,  66])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.random.choice(range(150), 75, replace=False)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   5,   7,   8,   9,  12,  13,  14,  16,  17,\n",
       "        19,  20,  22,  25,  26,  29,  30,  33,  35,  38,  40,  43,  46,\n",
       "        47,  53,  55,  56,  61,  67,  72,  75,  76,  77,  78,  81,  83,\n",
       "        84,  85,  86,  89,  90,  92,  93,  94,  95,  96,  97, 102, 103,\n",
       "       107, 109, 110, 114, 117, 118, 121, 123, 125, 127, 130, 131, 133,\n",
       "       136, 137, 138, 140, 141, 142, 143, 144, 148, 149])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.delete(range(150), train)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on regarde le résutat sur la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gnb.fit(iris.data[train,], iris.target[train,]).predict(iris.data[test,])\n",
    "100*(iris.target[test,] != y_pred).sum()/len(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Construire une fonction NB de paramètre A, B et nb qui répète nb tirages aléatoires avec i données pour la partie apprentissage et 150-i pour le test avec i allant de A à B. La fonction renvoie une liste de taille B-A+1 avec le pourcentage de prédiction exacte sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([98, 25,  3, 57, 53])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(150), 5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37.612612612612615,\n",
       " 66.893424036281189,\n",
       " 67.579908675799075,\n",
       " 60.0,\n",
       " 60.18518518518519,\n",
       " 37.995337995337991,\n",
       " 41.549295774647888,\n",
       " 29.787234042553191,\n",
       " 26.904761904761909,\n",
       " 40.28776978417266,\n",
       " 13.768115942028984,\n",
       " 14.355231143552309,\n",
       " 20.588235294117649,\n",
       " 4.9382716049382713,\n",
       " 9.4527363184079611,\n",
       " 18.295739348370926,\n",
       " 9.8484848484848477,\n",
       " 19.592875318066159,\n",
       " 8.7179487179487172,\n",
       " 4.909560723514212,\n",
       " 9.1145833333333339,\n",
       " 4.9868766404199469,\n",
       " 8.2010582010582009,\n",
       " 8.0,\n",
       " 8.064516129032258,\n",
       " 8.1300813008130088,\n",
       " 5.4644808743169397,\n",
       " 6.8870523415977969,\n",
       " 8.3333333333333339]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NB(A,B,nb):\n",
    "    res = []\n",
    "    for i in range(A,B+1):\n",
    "        temp = 0\n",
    "        for j in range(nb):\n",
    "            train = np.random.choice(range(150), i, replace=False)\n",
    "            test = np.delete(range(150), train)\n",
    "            y_pred = gnb.fit(iris.data[train,], iris.target[train,]).predict(iris.data[test,])\n",
    "            temp = temp + 100*(iris.target[test,] != y_pred).sum()/len(test)\n",
    "        res = res + [temp/nb]\n",
    "    return res\n",
    "NB(2,30,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Tracer sur un graphique le vecteur NB(A,B,10) avec A = 2 et B = 149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEyCAYAAADeAVWKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npkRERESGQGFKREREZAgUpkRERESG4P8DdBxe7Tl+XEEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd70eac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = 2\n",
    "B = 149\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.linspace(A,B,B-A+1), NB(A,B,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mon Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  9, 10, 11, 21, 22, 23, 24, 31, 33, 34, 35, 37, 38, 44, 45,\n",
       "        48, 67, 68, 69, 70, 73], dtype=int64),)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = iris.data[train,]\n",
    "data_train.shape\n",
    "target_train = iris.target[train,]\n",
    "id_y0 = np.where(target_train == 0)\n",
    "id_y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 23, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.07391304,  3.45652174,  1.50869565,  0.25217391]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_train[id_y0,].shape)\n",
    "np.mean(data_train[id_y0,],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Si Bayes est si naif que ça, pourquoi pas le construire nous-même ! En s'aidant du cours, construire la fonction MonNaiveBayes sur le modèle suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta(x,mu_hat,sigma_hat,pi_hat):\n",
    "    \"\"\"\n",
    "    x : valeur à laquelle sera évaluée delta, np.array  size (n,d)\n",
    "    mu_hat : moyenne des features, np.array size d\n",
    "    sigma_hat : std des featues, np.array size d\n",
    "    pi_hat : proportion parmi le train, np.float\n",
    "    \"\"\" \n",
    "    d = x.shape[0]\n",
    "    log_density_features = -d*np.log(np.sqrt(2*np.pi))-np.sum(np.log(sigma_hat))-np.sum((x-mu_hat)**2/(2*sigma_hat**2),axis=1)\n",
    "    log_proba = np.log(pi_hat)\n",
    "    return log_density_features+log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.7  2.8  6.7  2. ]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 6.7  3.3  5.7  2.1]\n",
      " [ 6.   2.2  5.   1.5]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 6.5  3.   5.8  2.2]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 5.1  3.3  1.7  0.5]]\n"
     ]
    }
   ],
   "source": [
    "x = data_train[0:10,:]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.2,2.1,-1.5,-1])\n",
    "print(x.shape[0])\n",
    "d = x.shape[0]\n",
    "id_y0 = np.where(target_train==0)\n",
    "mu_hat0 = np.mean(data_train[id_y0,],axis=1)\n",
    "sigma_hat0 = np.std(data_train[id_y0,],axis=1)\n",
    "pi_hat0 = data_train[id_y0,].shape[1]/data_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-298.58446581]\n"
     ]
    }
   ],
   "source": [
    "print(delta(x,mu_hat,sigma_hat,pi_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2  2.1 -1.5 -1. ]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(np.argmax(x)) #axis=1 max sur les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?np.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MonNaiveBayes(data_train, train_label, test_feature):\n",
    "    \"\"\"\n",
    "    fonction discriminante Naive Bayes    \n",
    "    \"\"\"\n",
    "    \n",
    "    target_values = np.unique(target_train)\n",
    "    n_train,d_train = data_train.shape\n",
    "    n_test,d_test = test_feature.shape\n",
    "    sigma_hat = []\n",
    "    mu_hat = []\n",
    "    pi_hat = []\n",
    "\n",
    "    for y in target_values:\n",
    "        print(y)\n",
    "        id_y = np.where(target_train == y)\n",
    "        mu_hat.append(np.mean(data_train[id_y,],axis=1))\n",
    "        sigma_hat.append(np.std(data_train[id_y,],axis=1))\n",
    "        pi_hat.append(data_train[id_y,].shape[1]/data_train.shape[0])\n",
    "    \n",
    "    delta_hat_test = np.zeros((n_test,target_values.shape[0]))\n",
    "    delta_hat_train = np.zeros((n_train,target_values.shape[0]))\n",
    "    k = 0\n",
    "    for y in target_values:\n",
    "        delta_hat_test[:,k] = delta(test_feature,mu_hat[k],sigma_hat[k],pi_hat[k])\n",
    "        delta_hat_train[:,k] = delta(data_train,mu_hat[k],sigma_hat[k],pi_hat[k])\n",
    "\n",
    "        k += 1\n",
    "        predict_test = np.argmax(delta_hat_test,axis=1)\n",
    "        predict_train = np.argmax(delta_hat_train,axis=1)\n",
    "        res = {\"predict_test\":predict_test,\n",
    "               \"predict_train\":predict_train}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = MonNaiveBayes(data_train, target_train, iris.data[test,])\n",
    "results[\"predict_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Comparer le résultat au résultat de la fonction de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MonNaiveBayes() missing 1 required positional argument: 'test_feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-719494bf4d6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0miris2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0miris2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mMonNaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miris2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: MonNaiveBayes() missing 1 required positional argument: 'test_feature'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "iris2 = copy.deepcopy(iris)\n",
    "\n",
    "iris2.data = iris2.data[3:30,]\n",
    "iris2.target = iris2.target[3:30,]\n",
    "MonNaiveBayes(iris, iris2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
